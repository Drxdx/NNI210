digraph {
	graph [size="258.59999999999997,258.59999999999997"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	139665592762176 [label="
 (1, 10)" fillcolor=darkolivegreen1]
	139665592737648 [label=AddmmBackward0]
	139665592737744 -> 139665592737648
	139665592966736 [label="classifier.bias
 (10)" fillcolor=lightblue]
	139665592966736 -> 139665592737744
	139665592737744 [label=AccumulateGrad]
	139665592737696 -> 139665592737648
	139665592737696 [label=ViewBackward0]
	139665592737552 -> 139665592737696
	139665592737552 [label=MeanBackward1]
	139665592737168 -> 139665592737552
	139665592737168 [label=CatBackward0]
	139665592836304 -> 139665592737168
	139665592836304 [label=SumBackward1]
	139665592836592 -> 139665592836304
	139665592836592 [label=StackBackward0]
	139665592836688 -> 139665592836592
	139665592836688 [label=NativeGroupNormBackward0]
	139665592836832 -> 139665592836688
	139665592836832 [label=ConvolutionBackward0]
	139665592836928 -> 139665592836832
	139665592836928 [label=ConvolutionBackward0]
	139665592837072 -> 139665592836928
	139665592837072 [label=ReluBackward0]
	139665592837216 -> 139665592837072
	139665592837216 [label=NativeGroupNormBackward0]
	139665592837312 -> 139665592837216
	139665592837312 [label=ConvolutionBackward0]
	139665592837408 -> 139665592837312
	139665592837408 [label=ConvolutionBackward0]
	139665592837552 -> 139665592837408
	139665592837552 [label=ReluBackward0]
	139665592837696 -> 139665592837552
	139665592837696 [label=NativeGroupNormBackward0]
	139665592837792 -> 139665592837696
	139665592837792 [label=CatBackward0]
	139665592837888 -> 139665592837792
	139665592837888 [label=ConvolutionBackward0]
	139665592838032 -> 139665592837888
	139665592838032 [label=ReluBackward0]
	139665592838176 -> 139665592838032
	139665592838176 [label=CatBackward0]
	139665592838272 -> 139665592838176
	139665592838272 [label=SumBackward1]
	139665592838560 -> 139665592838272
	139665592838560 [label=StackBackward0]
	139665592838656 -> 139665592838560
	139665592838656 [label=NativeGroupNormBackward0]
	139665592838800 -> 139665592838656
	139665592838800 [label=ConvolutionBackward0]
	139665592838896 -> 139665592838800
	139665592838896 [label=ConvolutionBackward0]
	139665592839040 -> 139665592838896
	139665592839040 [label=ReluBackward0]
	139665592839184 -> 139665592839040
	139665592839184 [label=NativeGroupNormBackward0]
	139665592839280 -> 139665592839184
	139665592839280 [label=ConvolutionBackward0]
	139665592839376 -> 139665592839280
	139665592839376 [label=ConvolutionBackward0]
	139665592839520 -> 139665592839376
	139665592839520 [label=ReluBackward0]
	139665592839664 -> 139665592839520
	139665592839664 [label=NativeGroupNormBackward0]
	139665592839760 -> 139665592839664
	139665592839760 [label=CatBackward0]
	139665592839856 -> 139665592839760
	139665592839856 [label=ConvolutionBackward0]
	139665592840000 -> 139665592839856
	139665592840000 [label=ReluBackward0]
	139665592840144 -> 139665592840000
	139665592840144 [label=CatBackward0]
	139665592840048 -> 139665592840144
	139665592840048 [label=SumBackward1]
	139665592852880 -> 139665592840048
	139665592852880 [label=StackBackward0]
	139665592852976 -> 139665592852880
	139665592852976 [label=NativeGroupNormBackward0]
	139665592853120 -> 139665592852976
	139665592853120 [label=ConvolutionBackward0]
	139665592853216 -> 139665592853120
	139665592853216 [label=ConvolutionBackward0]
	139665592853360 -> 139665592853216
	139665592853360 [label=ReluBackward0]
	139665592853504 -> 139665592853360
	139665592853504 [label=NativeGroupNormBackward0]
	139665592853600 -> 139665592853504
	139665592853600 [label=ConvolutionBackward0]
	139665592853696 -> 139665592853600
	139665592853696 [label=ConvolutionBackward0]
	139665592853840 -> 139665592853696
	139665592853840 [label=ReluBackward0]
	139665592853984 -> 139665592853840
	139665592853984 [label=NativeGroupNormBackward0]
	139665592854080 -> 139665592853984
	139665592854080 [label=ConvolutionBackward0]
	139665592854176 -> 139665592854080
	139665592854176 [label=ReluBackward0]
	139665592854320 -> 139665592854176
	139665592854320 [label=NativeGroupNormBackward0]
	139665592854416 -> 139665592854320
	139665592854416 [label=ConvolutionBackward0]
	139665592854512 -> 139665592854416
	139665595474464 [label="stem.0.weight
 (48, 3, 3, 3)" fillcolor=lightblue]
	139665595474464 -> 139665592854512
	139665592854512 [label=AccumulateGrad]
	139665592854128 -> 139665592854080
	139665595474624 [label="stages.0.0.preprocessor.pre0.1.weight
 (16, 48, 1, 1)" fillcolor=lightblue]
	139665595474624 -> 139665592854128
	139665592854128 [label=AccumulateGrad]
	139665592853792 -> 139665592853696
	139665595476224 [label="stages.0.0.ops.0.0.1.weight
 (16, 1, 5, 5)" fillcolor=lightblue]
	139665595476224 -> 139665592853792
	139665592853792 [label=AccumulateGrad]
	139665592853648 -> 139665592853600
	139665595476384 [label="stages.0.0.ops.0.0.2.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665595476384 -> 139665592853648
	139665592853648 [label=AccumulateGrad]
	139665592853312 -> 139665592853216
	139665595476544 [label="stages.0.0.ops.0.0.5.weight
 (16, 1, 5, 5)" fillcolor=lightblue]
	139665595476544 -> 139665592853312
	139665592853312 [label=AccumulateGrad]
	139665592853168 -> 139665592853120
	139665595476704 [label="stages.0.0.ops.0.0.6.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665595476704 -> 139665592853168
	139665592853168 [label=AccumulateGrad]
	139665592852928 -> 139665592852880
	139665592852928 [label=NativeGroupNormBackward0]
	139665592853552 -> 139665592852928
	139665592853552 [label=ConvolutionBackward0]
	139665592853264 -> 139665592853552
	139665592853264 [label=ConvolutionBackward0]
	139665592853888 -> 139665592853264
	139665592853888 [label=ReluBackward0]
	139665592854464 -> 139665592853888
	139665592854464 [label=NativeGroupNormBackward0]
	139665592854272 -> 139665592854464
	139665592854272 [label=ConvolutionBackward0]
	139665592854560 -> 139665592854272
	139665592854560 [label=ConvolutionBackward0]
	139665592854704 -> 139665592854560
	139665592854704 [label=ReluBackward0]
	139665592854848 -> 139665592854704
	139665592854848 [label=NativeGroupNormBackward0]
	139665592854944 -> 139665592854848
	139665592854944 [label=ConvolutionBackward0]
	139665592855040 -> 139665592854944
	139665592855040 [label=ReluBackward0]
	139665592854320 -> 139665592855040
	139665592854992 -> 139665592854944
	139665595474784 [label="stages.0.0.preprocessor.pre1.1.weight
 (16, 48, 1, 1)" fillcolor=lightblue]
	139665595474784 -> 139665592854992
	139665592854992 [label=AccumulateGrad]
	139665592854656 -> 139665592854560
	139665595220192 [label="stages.0.0.ops.0.1.1.weight
 (16, 1, 5, 5)" fillcolor=lightblue]
	139665595220192 -> 139665592854656
	139665592854656 [label=AccumulateGrad]
	139665592854608 -> 139665592854272
	139665595220352 [label="stages.0.0.ops.0.1.2.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665595220352 -> 139665592854608
	139665592854608 [label=AccumulateGrad]
	139665592854032 -> 139665592853264
	139665595220512 [label="stages.0.0.ops.0.1.5.weight
 (16, 1, 5, 5)" fillcolor=lightblue]
	139665595220512 -> 139665592854032
	139665592854032 [label=AccumulateGrad]
	139665592853408 -> 139665592853552
	139665595220672 [label="stages.0.0.ops.0.1.6.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665595220672 -> 139665592853408
	139665592853408 [label=AccumulateGrad]
	139665592852592 -> 139665592840144
	139665592852592 [label=SumBackward1]
	139665592853024 -> 139665592852592
	139665592853024 [label=StackBackward0]
	139665592854368 -> 139665592853024
	139665592854368 [label=NativeGroupNormBackward0]
	139665592853936 -> 139665592854368
	139665592853936 [label=ConvolutionBackward0]
	139665592854752 -> 139665592853936
	139665592854752 [label=ConvolutionBackward0]
	139665592855088 -> 139665592854752
	139665592855088 [label=ReluBackward0]
	139665592855232 -> 139665592855088
	139665592855232 [label=NativeGroupNormBackward0]
	139665592855328 -> 139665592855232
	139665592855328 [label=ConvolutionBackward0]
	139665592855424 -> 139665592855328
	139665592855424 [label=ConvolutionBackward0]
	139665592855568 -> 139665592855424
	139665592855568 [label=ReluBackward0]
	139665592854848 -> 139665592855568
	139665592855520 -> 139665592855424
	139665595477024 [label="stages.0.0.ops.1.0.1.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	139665595477024 -> 139665592855520
	139665592855520 [label=AccumulateGrad]
	139665592855376 -> 139665592855328
	139665595475904 [label="stages.0.0.ops.1.0.2.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665595475904 -> 139665592855376
	139665592855376 [label=AccumulateGrad]
	139665592854800 -> 139665592854752
	139665595475584 [label="stages.0.0.ops.1.0.5.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	139665595475584 -> 139665592854800
	139665592854800 [label=AccumulateGrad]
	139665592854896 -> 139665592853936
	139665595221072 [label="stages.0.0.ops.1.0.6.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665595221072 -> 139665592854896
	139665592854896 [label=AccumulateGrad]
	139665592853072 -> 139665592853024
	139665592853072 [label=NativeGroupNormBackward0]
	139665592855280 -> 139665592853072
	139665592855280 [label=ConvolutionBackward0]
	139665592854224 -> 139665592855280
	139665592854224 [label=ConvolutionBackward0]
	139665592855616 -> 139665592854224
	139665592855616 [label=ReluBackward0]
	139665592855760 -> 139665592855616
	139665592855760 [label=NativeGroupNormBackward0]
	139665592855856 -> 139665592855760
	139665592855856 [label=ConvolutionBackward0]
	139665592855952 -> 139665592855856
	139665592855952 [label=ConvolutionBackward0]
	139665592856096 -> 139665592855952
	139665592856096 [label=ReluBackward0]
	139665592853984 -> 139665592856096
	139665592856048 -> 139665592855952
	139665595477264 [label="stages.0.0.ops.1.1.1.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	139665595477264 -> 139665592856048
	139665592856048 [label=AccumulateGrad]
	139665592855904 -> 139665592855856
	139665595221312 [label="stages.0.0.ops.1.1.2.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665595221312 -> 139665592855904
	139665592855904 [label=AccumulateGrad]
	139665592855664 -> 139665592854224
	139665595220992 [label="stages.0.0.ops.1.1.5.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	139665595220992 -> 139665592855664
	139665592855664 [label=AccumulateGrad]
	139665592855136 -> 139665592855280
	139665595222352 [label="stages.0.0.ops.1.1.6.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665595222352 -> 139665592855136
	139665592855136 [label=AccumulateGrad]
	139665592852544 -> 139665592840144
	139665592852544 [label=SumBackward1]
	139665592853744 -> 139665592852544
	139665592853744 [label=StackBackward0]
	139665592855808 -> 139665592853744
	139665592855808 [label=NativeGroupNormBackward0]
	139665592855712 -> 139665592855808
	139665592855712 [label=ConvolutionBackward0]
	139665592856144 -> 139665592855712
	139665592856144 [label=ConvolutionBackward0]
	139665592856288 -> 139665592856144
	139665592856288 [label=ReluBackward0]
	139665592856432 -> 139665592856288
	139665592856432 [label=NativeGroupNormBackward0]
	139665592856528 -> 139665592856432
	139665592856528 [label=ConvolutionBackward0]
	139665592856336 -> 139665592856528
	139665592856336 [label=ConvolutionBackward0]
	139665592873216 -> 139665592856336
	139665592873216 [label=ReluBackward0]
	139665592853984 -> 139665592873216
	139665592873168 -> 139665592856336
	139665595222192 [label="stages.0.0.ops.2.0.1.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	139665595222192 -> 139665592873168
	139665592873168 [label=AccumulateGrad]
	139665592873072 -> 139665592856528
	139665595221952 [label="stages.0.0.ops.2.0.2.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665595221952 -> 139665592873072
	139665592873072 [label=AccumulateGrad]
	139665592856240 -> 139665592856144
	139665595221632 [label="stages.0.0.ops.2.0.5.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	139665595221632 -> 139665592856240
	139665592856240 [label=AccumulateGrad]
	139665592856192 -> 139665592855712
	139665595223632 [label="stages.0.0.ops.2.0.6.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665595223632 -> 139665592856192
	139665592856192 [label=AccumulateGrad]
	139665592853456 -> 139665592853744
	139665592853456 [label=NativeGroupNormBackward0]
	139665592856384 -> 139665592853456
	139665592856384 [label=ConvolutionBackward0]
	139665592856000 -> 139665592856384
	139665592856000 [label=ConvolutionBackward0]
	139665592873264 -> 139665592856000
	139665592873264 [label=ReluBackward0]
	139665592873408 -> 139665592873264
	139665592873408 [label=NativeGroupNormBackward0]
	139665592873504 -> 139665592873408
	139665592873504 [label=ConvolutionBackward0]
	139665592873600 -> 139665592873504
	139665592873600 [label=ConvolutionBackward0]
	139665592873744 -> 139665592873600
	139665592873744 [label=ReluBackward0]
	139665592854848 -> 139665592873744
	139665592873696 -> 139665592873600
	139665595223552 [label="stages.0.0.ops.2.1.1.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	139665595223552 -> 139665592873696
	139665592873696 [label=AccumulateGrad]
	139665592873552 -> 139665592873504
	139665595223072 [label="stages.0.0.ops.2.1.2.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665595223072 -> 139665592873552
	139665592873552 [label=AccumulateGrad]
	139665592873312 -> 139665592856000
	139665595222752 [label="stages.0.0.ops.2.1.5.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	139665595222752 -> 139665592873312
	139665592873312 [label=AccumulateGrad]
	139665592856480 -> 139665592856384
	139665594815408 [label="stages.0.0.ops.2.1.6.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665594815408 -> 139665592856480
	139665592856480 [label=AccumulateGrad]
	139665592852640 -> 139665592840144
	139665592852640 [label=SumBackward1]
	139665592855472 -> 139665592852640
	139665592855472 [label=StackBackward0]
	139665592855184 -> 139665592855472
	139665592855184 [label=MaxPool2DWithIndicesBackward0]
	139665592852592 -> 139665592855184
	139665592852832 -> 139665592855472
	139665592852832 [label=NativeGroupNormBackward0]
	139665592873120 -> 139665592852832
	139665592873120 [label=ConvolutionBackward0]
	139665592873792 -> 139665592873120
	139665592873792 [label=ConvolutionBackward0]
	139665592873936 -> 139665592873792
	139665592873936 [label=ReluBackward0]
	139665592874080 -> 139665592873936
	139665592874080 [label=NativeGroupNormBackward0]
	139665592874176 -> 139665592874080
	139665592874176 [label=ConvolutionBackward0]
	139665592874272 -> 139665592874176
	139665592874272 [label=ConvolutionBackward0]
	139665592874416 -> 139665592874272
	139665592874416 [label=ReluBackward0]
	139665592854848 -> 139665592874416
	139665592874368 -> 139665592874272
	139665594816528 [label="stages.0.0.ops.3.1.1.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	139665594816528 -> 139665592874368
	139665592874368 [label=AccumulateGrad]
	139665592874224 -> 139665592874176
	139665594816288 [label="stages.0.0.ops.3.1.2.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665594816288 -> 139665592874224
	139665592874224 [label=AccumulateGrad]
	139665592873888 -> 139665592873792
	139665594815968 [label="stages.0.0.ops.3.1.5.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	139665594815968 -> 139665592873888
	139665592873888 [label=AccumulateGrad]
	139665592873840 -> 139665592873120
	139665594817968 [label="stages.0.0.ops.3.1.6.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665594817968 -> 139665592873840
	139665592873840 [label=AccumulateGrad]
	139665592852688 -> 139665592840144
	139665592852688 [label=SumBackward1]
	139665592852784 -> 139665592852688
	139665592852784 [label=StackBackward0]
	139665592874128 -> 139665592852784
	139665592874128 [label=MaxPool2DWithIndicesBackward0]
	139665592852592 -> 139665592874128
	139665592873360 -> 139665592852784
	139665592873360 [label=NativeGroupNormBackward0]
	139665592873984 -> 139665592873360
	139665592873984 [label=ConvolutionBackward0]
	139665592874464 -> 139665592873984
	139665592874464 [label=ConvolutionBackward0]
	139665592874608 -> 139665592874464
	139665592874608 [label=ReluBackward0]
	139665592874752 -> 139665592874608
	139665592874752 [label=NativeGroupNormBackward0]
	139665592874848 -> 139665592874752
	139665592874848 [label=ConvolutionBackward0]
	139665592874944 -> 139665592874848
	139665592874944 [label=ConvolutionBackward0]
	139665592875088 -> 139665592874944
	139665592875088 [label=ReluBackward0]
	139665592854848 -> 139665592875088
	139665592875040 -> 139665592874944
	139665594758144 [label="stages.0.0.ops.4.1.1.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	139665594758144 -> 139665592875040
	139665592875040 [label=AccumulateGrad]
	139665592874896 -> 139665592874848
	139665594757824 [label="stages.0.0.ops.4.1.2.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665594757824 -> 139665592874896
	139665592874896 [label=AccumulateGrad]
	139665592874560 -> 139665592874464
	139665594757344 [label="stages.0.0.ops.4.1.5.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	139665594757344 -> 139665592874560
	139665592874560 [label=AccumulateGrad]
	139665592874512 -> 139665592873984
	139665594758864 [label="stages.0.0.ops.4.1.6.weight
 (16, 16, 1, 1)" fillcolor=lightblue]
	139665594758864 -> 139665592874512
	139665592874512 [label=AccumulateGrad]
	139665592839952 -> 139665592839856
	139665594304048 [label="stages.1.1.preprocessor.pre0.conv_1.weight
 (16, 80, 1, 1)" fillcolor=lightblue]
	139665594304048 -> 139665592839952
	139665592839952 [label=AccumulateGrad]
	139665592839808 -> 139665592839760
	139665592839808 [label=ConvolutionBackward0]
	139665592840096 -> 139665592839808
	139665592840096 [label=SliceBackward0]
	139665592873648 -> 139665592840096
	139665592873648 [label=SliceBackward0]
	139665592874800 -> 139665592873648
	139665592874800 [label=SliceBackward0]
	139665592874320 -> 139665592874800
	139665592874320 [label=SliceBackward0]
	139665592875184 -> 139665592874320
	139665592875184 [label=ConstantPadNdBackward0]
	139665592840000 -> 139665592875184
	139665592839904 -> 139665592839808
	139665594304608 [label="stages.1.1.preprocessor.pre0.conv_2.weight
 (16, 80, 1, 1)" fillcolor=lightblue]
	139665594304608 -> 139665592839904
	139665592839904 [label=AccumulateGrad]
	139665592839472 -> 139665592839376
	139665594259072 [label="stages.1.1.ops.0.0.1.weight
 (32, 1, 5, 5)" fillcolor=lightblue]
	139665594259072 -> 139665592839472
	139665592839472 [label=AccumulateGrad]
	139665592839328 -> 139665592839280
	139665594259232 [label="stages.1.1.ops.0.0.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594259232 -> 139665592839328
	139665592839328 [label=AccumulateGrad]
	139665592838992 -> 139665592838896
	139665594259392 [label="stages.1.1.ops.0.0.5.weight
 (32, 1, 5, 5)" fillcolor=lightblue]
	139665594259392 -> 139665592838992
	139665592838992 [label=AccumulateGrad]
	139665592838848 -> 139665592838800
	139665594259552 [label="stages.1.1.ops.0.0.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594259552 -> 139665592838848
	139665592838848 [label=AccumulateGrad]
	139665592838608 -> 139665592838560
	139665592838608 [label=NativeGroupNormBackward0]
	139665592839232 -> 139665592838608
	139665592839232 [label=ConvolutionBackward0]
	139665592838944 -> 139665592839232
	139665592838944 [label=ConvolutionBackward0]
	139665592839568 -> 139665592838944
	139665592839568 [label=ReluBackward0]
	139665592852736 -> 139665592839568
	139665592852736 [label=NativeGroupNormBackward0]
	139665592839424 -> 139665592852736
	139665592839424 [label=ConvolutionBackward0]
	139665592874704 -> 139665592839424
	139665592874704 [label=ConvolutionBackward0]
	139665592873024 -> 139665592874704
	139665592873024 [label=ReluBackward0]
	139665592875328 -> 139665592873024
	139665592875328 [label=NativeGroupNormBackward0]
	139665592875424 -> 139665592875328
	139665592875424 [label=ConvolutionBackward0]
	139665592875520 -> 139665592875424
	139665592875520 [label=ReluBackward0]
	139665592875664 -> 139665592875520
	139665592875664 [label=CatBackward0]
	139665592875760 -> 139665592875664
	139665592875760 [label=SumBackward1]
	139665592876048 -> 139665592875760
	139665592876048 [label=StackBackward0]
	139665592876144 -> 139665592876048
	139665592876144 [label=NativeGroupNormBackward0]
	139665592876288 -> 139665592876144
	139665592876288 [label=ConvolutionBackward0]
	139665592876384 -> 139665592876288
	139665592876384 [label=ConvolutionBackward0]
	139665592876528 -> 139665592876384
	139665592876528 [label=ReluBackward0]
	139665592876672 -> 139665592876528
	139665592876672 [label=NativeGroupNormBackward0]
	139665592876768 -> 139665592876672
	139665592876768 [label=ConvolutionBackward0]
	139665592876864 -> 139665592876768
	139665592876864 [label=ConvolutionBackward0]
	139665592877008 -> 139665592876864
	139665592877008 [label=ReluBackward0]
	139664412901584 -> 139665592877008
	139664412901584 [label=NativeGroupNormBackward0]
	139664412901680 -> 139664412901584
	139664412901680 [label=ConvolutionBackward0]
	139664412901776 -> 139664412901680
	139664412901776 [label=ReluBackward0]
	139665592854320 -> 139664412901776
	139664412901728 -> 139664412901680
	139665594814688 [label="stages.1.0.preprocessor.pre0.1.weight
 (32, 48, 1, 1)" fillcolor=lightblue]
	139665594814688 -> 139664412901728
	139664412901728 [label=AccumulateGrad]
	139665592876960 -> 139665592876864
	139665594758464 [label="stages.1.0.ops.0.0.1.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594758464 -> 139665592876960
	139665592876960 [label=AccumulateGrad]
	139665592876816 -> 139665592876768
	139665594757904 [label="stages.1.0.ops.0.0.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594757904 -> 139665592876816
	139665592876816 [label=AccumulateGrad]
	139665592876480 -> 139665592876384
	139665594760464 [label="stages.1.0.ops.0.0.5.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594760464 -> 139665592876480
	139665592876480 [label=AccumulateGrad]
	139665592876336 -> 139665592876288
	139665594760624 [label="stages.1.0.ops.0.0.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594760624 -> 139665592876336
	139665592876336 [label=AccumulateGrad]
	139665592876096 -> 139665592876048
	139665592876096 [label=NativeGroupNormBackward0]
	139665592876720 -> 139665592876096
	139665592876720 [label=CatBackward0]
	139665592876432 -> 139665592876720
	139665592876432 [label=ConvolutionBackward0]
	139665592876912 -> 139665592876432
	139665592876912 [label=NativeGroupNormBackward0]
	139664412901824 -> 139665592876912
	139664412901824 [label=ConvolutionBackward0]
	139664412901920 -> 139664412901824
	139664412901920 [label=ReluBackward0]
	139665592840144 -> 139664412901920
	139664412901872 -> 139664412901824
	139665594817408 [label="stages.1.0.preprocessor.pre1.1.weight
 (32, 80, 1, 1)" fillcolor=lightblue]
	139665594817408 -> 139664412901872
	139664412901872 [label=AccumulateGrad]
	139665592876624 -> 139665592876432
	139665594536880 [label="stages.1.0.ops.0.1.conv_1.weight
 (16, 32, 1, 1)" fillcolor=lightblue]
	139665594536880 -> 139665592876624
	139665592876624 [label=AccumulateGrad]
	139665592876576 -> 139665592876720
	139665592876576 [label=ConvolutionBackward0]
	139664412901488 -> 139665592876576
	139664412901488 [label=SliceBackward0]
	139664412902064 -> 139664412901488
	139664412902064 [label=SliceBackward0]
	139664412902160 -> 139664412902064
	139664412902160 [label=SliceBackward0]
	139664412902256 -> 139664412902160
	139664412902256 [label=SliceBackward0]
	139665592876912 -> 139664412902256
	139664412901968 -> 139665592876576
	139665594537040 [label="stages.1.0.ops.0.1.conv_2.weight
 (16, 32, 1, 1)" fillcolor=lightblue]
	139665594537040 -> 139664412901968
	139664412901968 [label=AccumulateGrad]
	139665592875712 -> 139665592875664
	139665592875712 [label=SumBackward1]
	139665592876192 -> 139665592875712
	139665592876192 [label=StackBackward0]
	139665592876240 -> 139665592876192
	139665592876240 [label=NativeGroupNormBackward0]
	139664412902016 -> 139665592876240
	139664412902016 [label=ConvolutionBackward0]
	139664412902304 -> 139664412902016
	139664412902304 [label=ConvolutionBackward0]
	139664412902448 -> 139664412902304
	139664412902448 [label=ReluBackward0]
	139664412902592 -> 139664412902448
	139664412902592 [label=NativeGroupNormBackward0]
	139664412902688 -> 139664412902592
	139664412902688 [label=ConvolutionBackward0]
	139664412902784 -> 139664412902688
	139664412902784 [label=ConvolutionBackward0]
	139664412902928 -> 139664412902784
	139664412902928 [label=ReluBackward0]
	139664412901584 -> 139664412902928
	139664412902880 -> 139664412902784
	139665594536720 [label="stages.1.0.ops.1.0.1.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594536720 -> 139664412902880
	139664412902880 [label=AccumulateGrad]
	139664412902736 -> 139664412902688
	139665594536400 [label="stages.1.0.ops.1.0.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594536400 -> 139664412902736
	139664412902736 [label=AccumulateGrad]
	139664412902400 -> 139664412902304
	139665594538720 [label="stages.1.0.ops.1.0.5.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594538720 -> 139664412902400
	139664412902400 [label=AccumulateGrad]
	139664412902352 -> 139664412902016
	139665594539120 [label="stages.1.0.ops.1.0.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594539120 -> 139664412902352
	139664412902352 [label=AccumulateGrad]
	139665592876000 -> 139665592876192
	139665592876000 [label=NativeGroupNormBackward0]
	139664412902640 -> 139665592876000
	139664412902640 [label=ConvolutionBackward0]
	139664412901536 -> 139664412902640
	139664412901536 [label=ConvolutionBackward0]
	139664412902976 -> 139664412901536
	139664412902976 [label=ReluBackward0]
	139664412903120 -> 139664412902976
	139664412903120 [label=NativeGroupNormBackward0]
	139664412903216 -> 139664412903120
	139664412903216 [label=ConvolutionBackward0]
	139664412903312 -> 139664412903216
	139664412903312 [label=ConvolutionBackward0]
	139664412903456 -> 139664412903312
	139664412903456 [label=ReluBackward0]
	139665592875760 -> 139664412903456
	139664412903408 -> 139664412903312
	139665594538640 [label="stages.1.0.ops.1.1.1.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594538640 -> 139664412903408
	139664412903408 [label=AccumulateGrad]
	139664412903264 -> 139664412903216
	139665594538160 [label="stages.1.0.ops.1.1.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594538160 -> 139664412903264
	139664412903264 [label=AccumulateGrad]
	139664412903024 -> 139664412901536
	139665594537840 [label="stages.1.0.ops.1.1.5.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594537840 -> 139664412903024
	139664412903024 [label=AccumulateGrad]
	139664412902496 -> 139664412902640
	139665594537520 [label="stages.1.0.ops.1.1.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594537520 -> 139664412902496
	139664412902496 [label=AccumulateGrad]
	139665592875568 -> 139665592875664
	139665592875568 [label=SumBackward1]
	139665592875952 -> 139665592875568
	139665592875952 [label=StackBackward0]
	139664412903168 -> 139665592875952
	139664412903168 [label=NativeGroupNormBackward0]
	139664412903072 -> 139664412903168
	139664412903072 [label=ConvolutionBackward0]
	139664412903504 -> 139664412903072
	139664412903504 [label=ConvolutionBackward0]
	139664412903648 -> 139664412903504
	139664412903648 [label=ReluBackward0]
	139664412903792 -> 139664412903648
	139664412903792 [label=NativeGroupNormBackward0]
	139664412903888 -> 139664412903792
	139664412903888 [label=ConvolutionBackward0]
	139664412903984 -> 139664412903888
	139664412903984 [label=ConvolutionBackward0]
	139664412904128 -> 139664412903984
	139664412904128 [label=ReluBackward0]
	139665592875712 -> 139664412904128
	139664412904080 -> 139664412903984
	139665594539600 [label="stages.1.0.ops.2.0.1.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594539600 -> 139664412904080
	139664412904080 [label=AccumulateGrad]
	139664412903936 -> 139664412903888
	139665594539280 [label="stages.1.0.ops.2.0.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594539280 -> 139664412903936
	139664412903936 [label=AccumulateGrad]
	139664412903600 -> 139664412903504
	139665594708352 [label="stages.1.0.ops.2.0.5.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594708352 -> 139664412903600
	139664412903600 [label=AccumulateGrad]
	139664412903552 -> 139664412903072
	139665594708032 [label="stages.1.0.ops.2.0.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594708032 -> 139664412903552
	139664412903552 [label=AccumulateGrad]
	139664412902112 -> 139665592875952
	139664412902112 [label=NativeGroupNormBackward0]
	139664412903840 -> 139664412902112
	139664412903840 [label=ConvolutionBackward0]
	139664412903360 -> 139664412903840
	139664412903360 [label=ConvolutionBackward0]
	139664412904176 -> 139664412903360
	139664412904176 [label=ReluBackward0]
	139664412904320 -> 139664412904176
	139664412904320 [label=NativeGroupNormBackward0]
	139664412904416 -> 139664412904320
	139664412904416 [label=ConvolutionBackward0]
	139664412904512 -> 139664412904416
	139664412904512 [label=ConvolutionBackward0]
	139664412904656 -> 139664412904512
	139664412904656 [label=ReluBackward0]
	139664412901584 -> 139664412904656
	139664412904608 -> 139664412904512
	139665594710992 [label="stages.1.0.ops.2.1.1.weight
 (32, 1, 5, 5)" fillcolor=lightblue]
	139665594710992 -> 139664412904608
	139664412904608 [label=AccumulateGrad]
	139664412904464 -> 139664412904416
	139665594711152 [label="stages.1.0.ops.2.1.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594711152 -> 139664412904464
	139664412904464 [label=AccumulateGrad]
	139664412904224 -> 139664412903360
	139665594711312 [label="stages.1.0.ops.2.1.5.weight
 (32, 1, 5, 5)" fillcolor=lightblue]
	139665594711312 -> 139664412904224
	139664412904224 [label=AccumulateGrad]
	139664412903696 -> 139664412903840
	139665594711472 [label="stages.1.0.ops.2.1.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594711472 -> 139664412903696
	139664412903696 [label=AccumulateGrad]
	139665592875808 -> 139665592875664
	139665592875808 [label=SumBackward1]
	139664412902832 -> 139665592875808
	139664412902832 [label=StackBackward0]
	139664412904368 -> 139664412902832
	139664412904368 [label=NativeGroupNormBackward0]
	139664412904272 -> 139664412904368
	139664412904272 [label=ConvolutionBackward0]
	139664412904704 -> 139664412904272
	139664412904704 [label=ConvolutionBackward0]
	139664412904848 -> 139664412904704
	139664412904848 [label=ReluBackward0]
	139664412904992 -> 139664412904848
	139664412904992 [label=NativeGroupNormBackward0]
	139664412905088 -> 139664412904992
	139664412905088 [label=ConvolutionBackward0]
	139664412905184 -> 139664412905088
	139664412905184 [label=ConvolutionBackward0]
	139664412905328 -> 139664412905184
	139664412905328 [label=ReluBackward0]
	139665592875568 -> 139664412905328
	139664412905280 -> 139664412905184
	139665594710112 [label="stages.1.0.ops.3.0.1.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594710112 -> 139664412905280
	139664412905280 [label=AccumulateGrad]
	139664412905136 -> 139664412905088
	139665594709872 [label="stages.1.0.ops.3.0.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594709872 -> 139664412905136
	139664412905136 [label=AccumulateGrad]
	139664412904800 -> 139664412904704
	139665594709552 [label="stages.1.0.ops.3.0.5.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594709552 -> 139664412904800
	139664412904800 [label=AccumulateGrad]
	139664412904752 -> 139664412904272
	139665594711872 [label="stages.1.0.ops.3.0.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594711872 -> 139664412904752
	139664412904752 [label=AccumulateGrad]
	139664412902544 -> 139664412902832
	139664412902544 [label=NativeGroupNormBackward0]
	139664412905040 -> 139664412902544
	139664412905040 [label=ConvolutionBackward0]
	139664412904560 -> 139664412905040
	139664412904560 [label=ConvolutionBackward0]
	139664412905376 -> 139664412904560
	139664412905376 [label=ReluBackward0]
	139664412905232 -> 139664412905376
	139664412905232 [label=NativeGroupNormBackward0]
	139664412934352 -> 139664412905232
	139664412934352 [label=ConvolutionBackward0]
	139664412934448 -> 139664412934352
	139664412934448 [label=ConvolutionBackward0]
	139664412934592 -> 139664412934448
	139664412934592 [label=ReluBackward0]
	139665592875712 -> 139664412934592
	139664412934544 -> 139664412934448
	139665594710672 [label="stages.1.0.ops.3.1.1.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594710672 -> 139664412934544
	139664412934544 [label=AccumulateGrad]
	139664412934400 -> 139664412934352
	139665594708592 [label="stages.1.0.ops.3.1.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594708592 -> 139664412934400
	139664412934400 [label=AccumulateGrad]
	139664412905424 -> 139664412904560
	139665594709392 [label="stages.1.0.ops.3.1.5.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594709392 -> 139664412905424
	139664412905424 [label=AccumulateGrad]
	139664412904896 -> 139664412905040
	139665594302528 [label="stages.1.0.ops.3.1.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594302528 -> 139664412904896
	139664412904896 [label=AccumulateGrad]
	139665592875856 -> 139665592875664
	139665592875856 [label=SumBackward1]
	139664412904032 -> 139665592875856
	139664412904032 [label=StackBackward0]
	139664412904944 -> 139664412904032
	139664412904944 [label=NativeGroupNormBackward0]
	139664412934256 -> 139664412904944
	139664412934256 [label=ConvolutionBackward0]
	139664412934640 -> 139664412934256
	139664412934640 [label=ConvolutionBackward0]
	139664412934784 -> 139664412934640
	139664412934784 [label=ReluBackward0]
	139664412934928 -> 139664412934784
	139664412934928 [label=NativeGroupNormBackward0]
	139664412935024 -> 139664412934928
	139664412935024 [label=ConvolutionBackward0]
	139664412935120 -> 139664412935024
	139664412935120 [label=ConvolutionBackward0]
	139664412935264 -> 139664412935120
	139664412935264 [label=ReluBackward0]
	139665592875568 -> 139664412935264
	139664412935216 -> 139664412935120
	139665594303488 [label="stages.1.0.ops.4.0.1.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594303488 -> 139664412935216
	139664412935216 [label=AccumulateGrad]
	139664412935072 -> 139664412935024
	139665594303248 [label="stages.1.0.ops.4.0.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594303248 -> 139664412935072
	139664412935072 [label=AccumulateGrad]
	139664412934736 -> 139664412934640
	139665594302928 [label="stages.1.0.ops.4.0.5.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594302928 -> 139664412934736
	139664412934736 [label=AccumulateGrad]
	139664412934688 -> 139664412934256
	139665594304768 [label="stages.1.0.ops.4.0.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594304768 -> 139664412934688
	139664412934688 [label=AccumulateGrad]
	139664412903744 -> 139664412904032
	139664412903744 [label=NativeGroupNormBackward0]
	139664412934976 -> 139664412903744
	139664412934976 [label=ConvolutionBackward0]
	139664412934496 -> 139664412934976
	139664412934496 [label=ConvolutionBackward0]
	139664412935312 -> 139664412934496
	139664412935312 [label=ReluBackward0]
	139664412935456 -> 139664412935312
	139664412935456 [label=NativeGroupNormBackward0]
	139664412935552 -> 139664412935456
	139664412935552 [label=ConvolutionBackward0]
	139664412935648 -> 139664412935552
	139664412935648 [label=ConvolutionBackward0]
	139664412935792 -> 139664412935648
	139664412935792 [label=ReluBackward0]
	139665592875808 -> 139664412935792
	139664412935744 -> 139664412935648
	139665594306448 [label="stages.1.0.ops.4.1.1.weight
 (32, 1, 5, 5)" fillcolor=lightblue]
	139665594306448 -> 139664412935744
	139664412935744 [label=AccumulateGrad]
	139664412935600 -> 139664412935552
	139665594257552 [label="stages.1.0.ops.4.1.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594257552 -> 139664412935600
	139664412935600 [label=AccumulateGrad]
	139664412935360 -> 139664412934496
	139665594257712 [label="stages.1.0.ops.4.1.5.weight
 (32, 1, 5, 5)" fillcolor=lightblue]
	139665594257712 -> 139664412935360
	139664412935360 [label=AccumulateGrad]
	139664412934832 -> 139664412934976
	139665594257872 [label="stages.1.0.ops.4.1.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594257872 -> 139664412934832
	139664412934832 [label=AccumulateGrad]
	139665592875472 -> 139665592875424
	139665594306288 [label="stages.1.1.preprocessor.pre1.1.weight
 (32, 160, 1, 1)" fillcolor=lightblue]
	139665594306288 -> 139665592875472
	139665592875472 [label=AccumulateGrad]
	139665592875136 -> 139665592874704
	139665594260992 [label="stages.1.1.ops.0.1.1.weight
 (32, 1, 5, 5)" fillcolor=lightblue]
	139665594260992 -> 139665592875136
	139665592875136 [label=AccumulateGrad]
	139665592873456 -> 139665592839424
	139665594261152 [label="stages.1.1.ops.0.1.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594261152 -> 139665592873456
	139665592873456 [label=AccumulateGrad]
	139665592839712 -> 139665592838944
	139665594261312 [label="stages.1.1.ops.0.1.5.weight
 (32, 1, 5, 5)" fillcolor=lightblue]
	139665594261312 -> 139665592839712
	139665592839712 [label=AccumulateGrad]
	139665592839088 -> 139665592839232
	139665594024000 [label="stages.1.1.ops.0.1.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594024000 -> 139665592839088
	139665592839088 [label=AccumulateGrad]
	139665592838224 -> 139665592838176
	139665592838224 [label=SumBackward1]
	139665592838704 -> 139665592838224
	139665592838704 [label=StackBackward0]
	139665592839616 -> 139665592838704
	139665592839616 [label=NativeGroupNormBackward0]
	139665592839136 -> 139665592839616
	139665592839136 [label=ConvolutionBackward0]
	139665592875232 -> 139665592839136
	139665592875232 [label=ConvolutionBackward0]
	139665592875904 -> 139665592875232
	139665592875904 [label=ReluBackward0]
	139664412901632 -> 139665592875904
	139664412901632 [label=NativeGroupNormBackward0]
	139665592875616 -> 139664412901632
	139665592875616 [label=ConvolutionBackward0]
	139664412935168 -> 139665592875616
	139664412935168 [label=ConvolutionBackward0]
	139664412935888 -> 139664412935168
	139664412935888 [label=ReluBackward0]
	139665592875328 -> 139664412935888
	139664412935408 -> 139664412935168
	139665594259872 [label="stages.1.1.ops.1.0.1.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594259872 -> 139664412935408
	139664412935408 [label=AccumulateGrad]
	139664412935504 -> 139665592875616
	139665594258752 [label="stages.1.1.ops.1.0.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594258752 -> 139664412935504
	139664412935504 [label=AccumulateGrad]
	139665592875280 -> 139665592875232
	139665594258192 [label="stages.1.1.ops.1.0.5.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594258192 -> 139665592875280
	139665592875280 [label=AccumulateGrad]
	139665592875376 -> 139665592839136
	139665594024400 [label="stages.1.1.ops.1.0.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594024400 -> 139665592875376
	139665592875376 [label=AccumulateGrad]
	139665592838752 -> 139665592838704
	139665592838752 [label=NativeGroupNormBackward0]
	139664412902208 -> 139665592838752
	139664412902208 [label=ConvolutionBackward0]
	139665592874992 -> 139664412902208
	139665592874992 [label=ConvolutionBackward0]
	139664412935840 -> 139665592874992
	139664412935840 [label=ReluBackward0]
	139664412935984 -> 139664412935840
	139664412935984 [label=NativeGroupNormBackward0]
	139664412936080 -> 139664412935984
	139664412936080 [label=ConvolutionBackward0]
	139664412936176 -> 139664412936080
	139664412936176 [label=ConvolutionBackward0]
	139664412936320 -> 139664412936176
	139664412936320 [label=ReluBackward0]
	139665592839664 -> 139664412936320
	139664412936272 -> 139664412936176
	139665594260352 [label="stages.1.1.ops.1.1.1.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594260352 -> 139664412936272
	139664412936272 [label=AccumulateGrad]
	139664412936128 -> 139664412936080
	139665594024560 [label="stages.1.1.ops.1.1.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594024560 -> 139664412936128
	139664412936128 [label=AccumulateGrad]
	139664412935696 -> 139665592874992
	139665594024240 [label="stages.1.1.ops.1.1.5.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594024240 -> 139664412935696
	139664412935696 [label=AccumulateGrad]
	139665592874032 -> 139664412902208
	139665594025680 [label="stages.1.1.ops.1.1.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594025680 -> 139665592874032
	139665592874032 [label=AccumulateGrad]
	139665592838080 -> 139665592838176
	139665592838080 [label=SumBackward1]
	139665592874656 -> 139665592838080
	139665592874656 [label=StackBackward0]
	139665592838464 -> 139665592874656
	139665592838464 [label=NativeGroupNormBackward0]
	139664412935936 -> 139665592838464
	139664412935936 [label=ConvolutionBackward0]
	139664412936368 -> 139664412935936
	139664412936368 [label=ConvolutionBackward0]
	139664412936512 -> 139664412936368
	139664412936512 [label=ReluBackward0]
	139664412936656 -> 139664412936512
	139664412936656 [label=NativeGroupNormBackward0]
	139664412936752 -> 139664412936656
	139664412936752 [label=ConvolutionBackward0]
	139664412936848 -> 139664412936752
	139664412936848 [label=ConvolutionBackward0]
	139664412936992 -> 139664412936848
	139664412936992 [label=ReluBackward0]
	139665592839664 -> 139664412936992
	139664412936944 -> 139664412936848
	139665594025520 [label="stages.1.1.ops.2.0.1.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594025520 -> 139664412936944
	139664412936944 [label=AccumulateGrad]
	139664412936800 -> 139664412936752
	139665594025280 [label="stages.1.1.ops.2.0.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594025280 -> 139664412936800
	139664412936800 [label=AccumulateGrad]
	139664412936464 -> 139664412936368
	139665594024960 [label="stages.1.1.ops.2.0.5.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594024960 -> 139664412936464
	139664412936464 [label=AccumulateGrad]
	139664412936416 -> 139664412935936
	139665594026960 [label="stages.1.1.ops.2.0.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594026960 -> 139664412936416
	139664412936416 [label=AccumulateGrad]
	139664412936032 -> 139665592874656
	139664412936032 [label=NativeGroupNormBackward0]
	139664412936704 -> 139664412936032
	139664412936704 [label=ConvolutionBackward0]
	139664412936224 -> 139664412936704
	139664412936224 [label=ConvolutionBackward0]
	139664412937040 -> 139664412936224
	139664412937040 [label=ReluBackward0]
	139664412937184 -> 139664412937040
	139664412937184 [label=NativeGroupNormBackward0]
	139664412937280 -> 139664412937184
	139664412937280 [label=ConvolutionBackward0]
	139664412937376 -> 139664412937280
	139664412937376 [label=ConvolutionBackward0]
	139664412937520 -> 139664412937376
	139664412937520 [label=ReluBackward0]
	139665592875328 -> 139664412937520
	139664412937472 -> 139664412937376
	139665594026880 [label="stages.1.1.ops.2.1.1.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594026880 -> 139664412937472
	139664412937472 [label=AccumulateGrad]
	139664412937328 -> 139664412937280
	139665594026400 [label="stages.1.1.ops.2.1.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594026400 -> 139664412937328
	139664412937328 [label=AccumulateGrad]
	139664412937088 -> 139664412936224
	139665594026080 [label="stages.1.1.ops.2.1.5.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594026080 -> 139664412937088
	139664412937088 [label=AccumulateGrad]
	139664412936560 -> 139664412936704
	139665594171696 [label="stages.1.1.ops.2.1.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594171696 -> 139664412936560
	139664412936560 [label=AccumulateGrad]
	139665592838320 -> 139665592838176
	139665592838320 [label=SumBackward1]
	139665592838512 -> 139665592838320
	139665592838512 [label=StackBackward0]
	139664412937232 -> 139665592838512
	139664412937232 [label=MaxPool2DWithIndicesBackward0]
	139665592838224 -> 139664412937232
	139664412934208 -> 139665592838512
	139664412934208 [label=NativeGroupNormBackward0]
	139664412936896 -> 139664412934208
	139664412936896 [label=ConvolutionBackward0]
	139664412937568 -> 139664412936896
	139664412937568 [label=ConvolutionBackward0]
	139664412937712 -> 139664412937568
	139664412937712 [label=ReluBackward0]
	139664412937856 -> 139664412937712
	139664412937856 [label=NativeGroupNormBackward0]
	139664412937952 -> 139664412937856
	139664412937952 [label=ConvolutionBackward0]
	139664412938048 -> 139664412937952
	139664412938048 [label=ConvolutionBackward0]
	139664412938192 -> 139664412938048
	139664412938192 [label=ReluBackward0]
	139665592875328 -> 139664412938192
	139664412938144 -> 139664412938048
	139665594172816 [label="stages.1.1.ops.3.1.1.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594172816 -> 139664412938144
	139664412938144 [label=AccumulateGrad]
	139664412938000 -> 139664412937952
	139665594172576 [label="stages.1.1.ops.3.1.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594172576 -> 139664412938000
	139664412938000 [label=AccumulateGrad]
	139664412937664 -> 139664412937568
	139665594172256 [label="stages.1.1.ops.3.1.5.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594172256 -> 139664412937664
	139664412937664 [label=AccumulateGrad]
	139664412937616 -> 139664412936896
	139665594174256 [label="stages.1.1.ops.3.1.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594174256 -> 139664412937616
	139664412937616 [label=AccumulateGrad]
	139665592838368 -> 139665592838176
	139665592838368 [label=SumBackward1]
	139664412936608 -> 139665592838368
	139664412936608 [label=StackBackward0]
	139664412937904 -> 139664412936608
	139664412937904 [label=MaxPool2DWithIndicesBackward0]
	139665592838224 -> 139664412937904
	139664412937136 -> 139664412936608
	139664412937136 [label=NativeGroupNormBackward0]
	139664412937760 -> 139664412937136
	139664412937760 [label=ConvolutionBackward0]
	139664412938096 -> 139664412937760
	139664412938096 [label=ConvolutionBackward0]
	139664412971264 -> 139664412938096
	139664412971264 [label=ReluBackward0]
	139664412971408 -> 139664412971264
	139664412971408 [label=NativeGroupNormBackward0]
	139664412971504 -> 139664412971408
	139664412971504 [label=ConvolutionBackward0]
	139664412971600 -> 139664412971504
	139664412971600 [label=ConvolutionBackward0]
	139664412971744 -> 139664412971600
	139664412971744 [label=ReluBackward0]
	139665592875328 -> 139664412971744
	139664412971696 -> 139664412971600
	139665594174816 [label="stages.1.1.ops.4.1.1.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665594174816 -> 139664412971696
	139664412971696 [label=AccumulateGrad]
	139664412971552 -> 139664412971504
	139665594174416 [label="stages.1.1.ops.4.1.2.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665594174416 -> 139664412971552
	139664412971552 [label=AccumulateGrad]
	139664412971120 -> 139664412938096
	139665593774464 [label="stages.1.1.ops.4.1.5.weight
 (32, 1, 3, 3)" fillcolor=lightblue]
	139665593774464 -> 139664412971120
	139664412971120 [label=AccumulateGrad]
	139664412937808 -> 139664412937760
	139665593775184 [label="stages.1.1.ops.4.1.6.weight
 (32, 32, 1, 1)" fillcolor=lightblue]
	139665593775184 -> 139664412937808
	139664412937808 [label=AccumulateGrad]
	139665592837984 -> 139665592837888
	139665593492560 [label="stages.2.1.preprocessor.pre0.conv_1.weight
 (32, 160, 1, 1)" fillcolor=lightblue]
	139665593492560 -> 139665592837984
	139665592837984 [label=AccumulateGrad]
	139665592837840 -> 139665592837792
	139665592837840 [label=ConvolutionBackward0]
	139665592838128 -> 139665592837840
	139665592838128 [label=SliceBackward0]
	139664412937424 -> 139665592838128
	139664412937424 [label=SliceBackward0]
	139664412934304 -> 139664412937424
	139664412934304 [label=SliceBackward0]
	139664412971216 -> 139664412934304
	139664412971216 [label=SliceBackward0]
	139664412971840 -> 139664412971216
	139664412971840 [label=ConstantPadNdBackward0]
	139665592838032 -> 139664412971840
	139665592838416 -> 139665592837840
	139665593493120 [label="stages.2.1.preprocessor.pre0.conv_2.weight
 (32, 160, 1, 1)" fillcolor=lightblue]
	139665593493120 -> 139665592838416
	139665592838416 [label=AccumulateGrad]
	139665592837504 -> 139665592837408
	139665593558176 [label="stages.2.1.ops.0.0.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	139665593558176 -> 139665592837504
	139665592837504 [label=AccumulateGrad]
	139665592837360 -> 139665592837312
	139665593558336 [label="stages.2.1.ops.0.0.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593558336 -> 139665592837360
	139665592837360 [label=AccumulateGrad]
	139665592837024 -> 139665592836928
	139665593558496 [label="stages.2.1.ops.0.0.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	139665593558496 -> 139665592837024
	139665592837024 [label=AccumulateGrad]
	139665592836880 -> 139665592836832
	139665593558656 [label="stages.2.1.ops.0.0.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593558656 -> 139665592836880
	139665592836880 [label=AccumulateGrad]
	139665592836640 -> 139665592836592
	139665592836640 [label=NativeGroupNormBackward0]
	139665592837264 -> 139665592836640
	139665592837264 [label=ConvolutionBackward0]
	139665592836976 -> 139665592837264
	139665592836976 [label=ConvolutionBackward0]
	139665592837600 -> 139665592836976
	139665592837600 [label=ReluBackward0]
	139665592837936 -> 139665592837600
	139665592837936 [label=NativeGroupNormBackward0]
	139664412934880 -> 139665592837936
	139664412934880 [label=ConvolutionBackward0]
	139664412971360 -> 139664412934880
	139664412971360 [label=ConvolutionBackward0]
	139664412971072 -> 139664412971360
	139664412971072 [label=ReluBackward0]
	139664412971984 -> 139664412971072
	139664412971984 [label=NativeGroupNormBackward0]
	139664412972080 -> 139664412971984
	139664412972080 [label=ConvolutionBackward0]
	139664412972176 -> 139664412972080
	139664412972176 [label=ReluBackward0]
	139664412972320 -> 139664412972176
	139664412972320 [label=CatBackward0]
	139664412972416 -> 139664412972320
	139664412972416 [label=SumBackward1]
	139664412972704 -> 139664412972416
	139664412972704 [label=StackBackward0]
	139664412972800 -> 139664412972704
	139664412972800 [label=NativeGroupNormBackward0]
	139664412972944 -> 139664412972800
	139664412972944 [label=ConvolutionBackward0]
	139664412973040 -> 139664412972944
	139664412973040 [label=ConvolutionBackward0]
	139664412973184 -> 139664412973040
	139664412973184 [label=ReluBackward0]
	139664412973328 -> 139664412973184
	139664412973328 [label=NativeGroupNormBackward0]
	139664412973424 -> 139664412973328
	139664412973424 [label=ConvolutionBackward0]
	139664412973520 -> 139664412973424
	139664412973520 [label=ConvolutionBackward0]
	139664412973664 -> 139664412973520
	139664412973664 [label=ReluBackward0]
	139664412973808 -> 139664412973664
	139664412973808 [label=NativeGroupNormBackward0]
	139664412973904 -> 139664412973808
	139664412973904 [label=ConvolutionBackward0]
	139664412974000 -> 139664412973904
	139664412974000 [label=ReluBackward0]
	139665592875664 -> 139664412974000
	139664412973952 -> 139664412973904
	139665594171536 [label="stages.2.0.preprocessor.pre0.1.weight
 (64, 160, 1, 1)" fillcolor=lightblue]
	139665594171536 -> 139664412973952
	139664412973952 [label=AccumulateGrad]
	139664412973616 -> 139664412973520
	139665593774784 [label="stages.2.0.ops.0.0.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593774784 -> 139664412973616
	139664412973616 [label=AccumulateGrad]
	139664412973472 -> 139664412973424
	139665593774224 [label="stages.2.0.ops.0.0.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593774224 -> 139664412973472
	139664412973472 [label=AccumulateGrad]
	139664412973136 -> 139664412973040
	139665593776944 [label="stages.2.0.ops.0.0.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593776944 -> 139664412973136
	139664412973136 [label=AccumulateGrad]
	139664412972992 -> 139664412972944
	139665593777104 [label="stages.2.0.ops.0.0.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593777104 -> 139664412972992
	139664412972992 [label=AccumulateGrad]
	139664412972752 -> 139664412972704
	139664412972752 [label=NativeGroupNormBackward0]
	139664412973376 -> 139664412972752
	139664412973376 [label=CatBackward0]
	139664412973088 -> 139664412973376
	139664412973088 [label=ConvolutionBackward0]
	139664412973712 -> 139664412973088
	139664412973712 [label=NativeGroupNormBackward0]
	139664412974048 -> 139664412973712
	139664412974048 [label=ConvolutionBackward0]
	139664412974144 -> 139664412974048
	139664412974144 [label=ReluBackward0]
	139665592838176 -> 139664412974144
	139664412974096 -> 139664412974048
	139665594173696 [label="stages.2.0.preprocessor.pre1.1.weight
 (64, 160, 1, 1)" fillcolor=lightblue]
	139665594173696 -> 139664412974096
	139664412974096 [label=AccumulateGrad]
	139664412973856 -> 139664412973088
	139665593811408 [label="stages.2.0.ops.0.1.conv_1.weight
 (32, 64, 1, 1)" fillcolor=lightblue]
	139665593811408 -> 139664412973856
	139664412973856 [label=AccumulateGrad]
	139664412973232 -> 139664412973376
	139664412973232 [label=ConvolutionBackward0]
	139664412973568 -> 139664412973232
	139664412973568 [label=SliceBackward0]
	139664412974288 -> 139664412973568
	139664412974288 [label=SliceBackward0]
	139664412974384 -> 139664412974288
	139664412974384 [label=SliceBackward0]
	139664412974480 -> 139664412974384
	139664412974480 [label=SliceBackward0]
	139664412973712 -> 139664412974480
	139664412974192 -> 139664412973232
	139665593811568 [label="stages.2.0.ops.0.1.conv_2.weight
 (32, 64, 1, 1)" fillcolor=lightblue]
	139665593811568 -> 139664412974192
	139664412974192 [label=AccumulateGrad]
	139664412972368 -> 139664412972320
	139664412972368 [label=SumBackward1]
	139664412972848 -> 139664412972368
	139664412972848 [label=StackBackward0]
	139664412973280 -> 139664412972848
	139664412973280 [label=NativeGroupNormBackward0]
	139664412974240 -> 139664412973280
	139664412974240 [label=ConvolutionBackward0]
	139664412974528 -> 139664412974240
	139664412974528 [label=ConvolutionBackward0]
	139664412974672 -> 139664412974528
	139664412974672 [label=ReluBackward0]
	139664412974816 -> 139664412974672
	139664412974816 [label=NativeGroupNormBackward0]
	139664412974912 -> 139664412974816
	139664412974912 [label=ConvolutionBackward0]
	139664412975008 -> 139664412974912
	139664412975008 [label=ConvolutionBackward0]
	139664412975056 -> 139664412975008
	139664412975056 [label=ReluBackward0]
	139664412973808 -> 139664412975056
	139664412995696 -> 139664412975008
	139665593775344 [label="stages.2.0.ops.1.0.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593775344 -> 139664412995696
	139664412995696 [label=AccumulateGrad]
	139664412974960 -> 139664412974912
	139665593811168 [label="stages.2.0.ops.1.0.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593811168 -> 139664412974960
	139664412974960 [label=AccumulateGrad]
	139664412974624 -> 139664412974528
	139665593813248 [label="stages.2.0.ops.1.0.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593813248 -> 139664412974624
	139664412974624 [label=AccumulateGrad]
	139664412974576 -> 139664412974240
	139665593813648 [label="stages.2.0.ops.1.0.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593813648 -> 139664412974576
	139664412974576 [label=AccumulateGrad]
	139664412972896 -> 139664412972848
	139664412972896 [label=NativeGroupNormBackward0]
	139664412974864 -> 139664412972896
	139664412974864 [label=ConvolutionBackward0]
	139664412973760 -> 139664412974864
	139664412973760 [label=ConvolutionBackward0]
	139664412974768 -> 139664412973760
	139664412974768 [label=ReluBackward0]
	139664412995888 -> 139664412974768
	139664412995888 [label=NativeGroupNormBackward0]
	139664412995984 -> 139664412995888
	139664412995984 [label=ConvolutionBackward0]
	139664412996080 -> 139664412995984
	139664412996080 [label=ConvolutionBackward0]
	139664412996224 -> 139664412996080
	139664412996224 [label=ReluBackward0]
	139664412972416 -> 139664412996224
	139664412996176 -> 139664412996080
	139665593813088 [label="stages.2.0.ops.1.1.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593813088 -> 139664412996176
	139664412996176 [label=AccumulateGrad]
	139664412996032 -> 139664412995984
	139665593812848 [label="stages.2.0.ops.1.1.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593812848 -> 139664412996032
	139664412996032 [label=AccumulateGrad]
	139664412995744 -> 139664412973760
	139665593812528 [label="stages.2.0.ops.1.1.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593812528 -> 139664412995744
	139664412995744 [label=AccumulateGrad]
	139664412974720 -> 139664412974864
	139665593812208 [label="stages.2.0.ops.1.1.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593812208 -> 139664412974720
	139664412974720 [label=AccumulateGrad]
	139664412972224 -> 139664412972320
	139664412972224 [label=SumBackward1]
	139664412974432 -> 139664412972224
	139664412974432 [label=StackBackward0]
	139664412974336 -> 139664412974432
	139664412974336 [label=NativeGroupNormBackward0]
	139664412995840 -> 139664412974336
	139664412995840 [label=ConvolutionBackward0]
	139664412996272 -> 139664412995840
	139664412996272 [label=ConvolutionBackward0]
	139664412996416 -> 139664412996272
	139664412996416 [label=ReluBackward0]
	139664412996560 -> 139664412996416
	139664412996560 [label=NativeGroupNormBackward0]
	139664412996656 -> 139664412996560
	139664412996656 [label=ConvolutionBackward0]
	139664412996752 -> 139664412996656
	139664412996752 [label=ConvolutionBackward0]
	139664412996896 -> 139664412996752
	139664412996896 [label=ReluBackward0]
	139664412972368 -> 139664412996896
	139664412996848 -> 139664412996752
	139665593814608 [label="stages.2.0.ops.2.0.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593814608 -> 139664412996848
	139664412996848 [label=AccumulateGrad]
	139664412996704 -> 139664412996656
	139665593814128 [label="stages.2.0.ops.2.0.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593814128 -> 139664412996704
	139664412996704 [label=AccumulateGrad]
	139664412996368 -> 139664412996272
	139665593813808 [label="stages.2.0.ops.2.0.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593813808 -> 139664412996368
	139664412996368 [label=AccumulateGrad]
	139664412996320 -> 139664412995840
	139665593777584 [label="stages.2.0.ops.2.0.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593777584 -> 139664412996320
	139664412996320 [label=AccumulateGrad]
	139664412972608 -> 139664412974432
	139664412972608 [label=NativeGroupNormBackward0]
	139664412996608 -> 139664412972608
	139664412996608 [label=ConvolutionBackward0]
	139664412996128 -> 139664412996608
	139664412996128 [label=ConvolutionBackward0]
	139664412996944 -> 139664412996128
	139664412996944 [label=ReluBackward0]
	139664412997088 -> 139664412996944
	139664412997088 [label=NativeGroupNormBackward0]
	139664412997184 -> 139664412997088
	139664412997184 [label=ConvolutionBackward0]
	139664412997280 -> 139664412997184
	139664412997280 [label=ConvolutionBackward0]
	139664412997424 -> 139664412997280
	139664412997424 [label=ReluBackward0]
	139664412973808 -> 139664412997424
	139664412997376 -> 139664412997280
	139665593911792 [label="stages.2.0.ops.2.1.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	139665593911792 -> 139664412997376
	139664412997376 [label=AccumulateGrad]
	139664412997232 -> 139664412997184
	139665593911952 [label="stages.2.0.ops.2.1.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593911952 -> 139664412997232
	139664412997232 [label=AccumulateGrad]
	139664412996992 -> 139664412996128
	139665593912112 [label="stages.2.0.ops.2.1.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	139665593912112 -> 139664412996992
	139664412996992 [label=AccumulateGrad]
	139664412996464 -> 139664412996608
	139665593912272 [label="stages.2.0.ops.2.1.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593912272 -> 139664412996464
	139664412996464 [label=AccumulateGrad]
	139664412972464 -> 139664412972320
	139664412972464 [label=SumBackward1]
	139664412972656 -> 139664412972464
	139664412972656 [label=StackBackward0]
	139664412997136 -> 139664412972656
	139664412997136 [label=NativeGroupNormBackward0]
	139664412997040 -> 139664412997136
	139664412997040 [label=ConvolutionBackward0]
	139664412997472 -> 139664412997040
	139664412997472 [label=ConvolutionBackward0]
	139664412997616 -> 139664412997472
	139664412997616 [label=ReluBackward0]
	139664412997760 -> 139664412997616
	139664412997760 [label=NativeGroupNormBackward0]
	139664412997856 -> 139664412997760
	139664412997856 [label=ConvolutionBackward0]
	139664412997952 -> 139664412997856
	139664412997952 [label=ConvolutionBackward0]
	139664412998096 -> 139664412997952
	139664412998096 [label=ReluBackward0]
	139664412972224 -> 139664412998096
	139664412998048 -> 139664412997952
	139665593910912 [label="stages.2.0.ops.3.0.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593910912 -> 139664412998048
	139664412998048 [label=AccumulateGrad]
	139664412997904 -> 139664412997856
	139665593910672 [label="stages.2.0.ops.3.0.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593910672 -> 139664412997904
	139664412997904 [label=AccumulateGrad]
	139664412997568 -> 139664412997472
	139665593910352 [label="stages.2.0.ops.3.0.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593910352 -> 139664412997568
	139664412997568 [label=AccumulateGrad]
	139664412997520 -> 139664412997040
	139665593912672 [label="stages.2.0.ops.3.0.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593912672 -> 139664412997520
	139664412997520 [label=AccumulateGrad]
	139664412995792 -> 139664412972656
	139664412995792 [label=NativeGroupNormBackward0]
	139664412997808 -> 139664412995792
	139664412997808 [label=ConvolutionBackward0]
	139664412997328 -> 139664412997808
	139664412997328 [label=ConvolutionBackward0]
	139664412998144 -> 139664412997328
	139664412998144 [label=ReluBackward0]
	139664412998288 -> 139664412998144
	139664412998288 [label=NativeGroupNormBackward0]
	139664412998384 -> 139664412998288
	139664412998384 [label=ConvolutionBackward0]
	139664412998480 -> 139664412998384
	139664412998480 [label=ConvolutionBackward0]
	139664412998624 -> 139664412998480
	139664412998624 [label=ReluBackward0]
	139664412972368 -> 139664412998624
	139664412998576 -> 139664412998480
	139665593912592 [label="stages.2.0.ops.3.1.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593912592 -> 139664412998576
	139664412998576 [label=AccumulateGrad]
	139664412998432 -> 139664412998384
	139665593911472 [label="stages.2.0.ops.3.1.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593911472 -> 139664412998432
	139664412998432 [label=AccumulateGrad]
	139664412998192 -> 139664412997328
	139665593909552 [label="stages.2.0.ops.3.1.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593909552 -> 139664412998192
	139664412998192 [label=AccumulateGrad]
	139664412997664 -> 139664412997808
	139665593910112 [label="stages.2.0.ops.3.1.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593910112 -> 139664412997664
	139664412997664 [label=AccumulateGrad]
	139664412972512 -> 139664412972320
	139664412972512 [label=SumBackward1]
	139664412996800 -> 139664412972512
	139664412996800 [label=StackBackward0]
	139664412998336 -> 139664412996800
	139664412998336 [label=NativeGroupNormBackward0]
	139664412998240 -> 139664412998336
	139664412998240 [label=ConvolutionBackward0]
	139664412998672 -> 139664412998240
	139664412998672 [label=ConvolutionBackward0]
	139664412998816 -> 139664412998672
	139664412998816 [label=ReluBackward0]
	139664412998960 -> 139664412998816
	139664412998960 [label=NativeGroupNormBackward0]
	139664412999056 -> 139664412998960
	139664412999056 [label=ConvolutionBackward0]
	139664412999152 -> 139664412999056
	139664412999152 [label=ConvolutionBackward0]
	139664412999296 -> 139664412999152
	139664412999296 [label=ReluBackward0]
	139664412972224 -> 139664412999296
	139664412999248 -> 139664412999152
	139665593492320 [label="stages.2.0.ops.4.0.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593492320 -> 139664412999248
	139664412999248 [label=AccumulateGrad]
	139664412999104 -> 139664412999056
	139665593492000 [label="stages.2.0.ops.4.0.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593492000 -> 139664412999104
	139664412999104 [label=AccumulateGrad]
	139664412998768 -> 139664412998672
	139665593491760 [label="stages.2.0.ops.4.0.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593491760 -> 139664412998768
	139664412998768 [label=AccumulateGrad]
	139664412998720 -> 139664412998240
	139665593493280 [label="stages.2.0.ops.4.0.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593493280 -> 139664412998720
	139664412998720 [label=AccumulateGrad]
	139664412996512 -> 139664412996800
	139664412996512 [label=NativeGroupNormBackward0]
	139664412999008 -> 139664412996512
	139664412999008 [label=ConvolutionBackward0]
	139664412998528 -> 139664412999008
	139664412998528 [label=ConvolutionBackward0]
	139664412999344 -> 139664412998528
	139664412999344 [label=ReluBackward0]
	139664412999488 -> 139664412999344
	139664412999488 [label=NativeGroupNormBackward0]
	139664412999584 -> 139664412999488
	139664412999584 [label=ConvolutionBackward0]
	139664412999632 -> 139664412999584
	139664412999632 [label=ConvolutionBackward0]
	139664413024464 -> 139664412999632
	139664413024464 [label=ReluBackward0]
	139664412972464 -> 139664413024464
	139664413024416 -> 139664412999632
	139665593494960 [label="stages.2.0.ops.4.1.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	139665593494960 -> 139664413024416
	139664413024416 [label=AccumulateGrad]
	139664412999200 -> 139664412999584
	139665593495120 [label="stages.2.0.ops.4.1.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593495120 -> 139664412999200
	139664412999200 [label=AccumulateGrad]
	139664412999392 -> 139664412998528
	139665593495280 [label="stages.2.0.ops.4.1.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	139665593495280 -> 139664412999392
	139664412999392 [label=AccumulateGrad]
	139664412998864 -> 139664412999008
	139665593495440 [label="stages.2.0.ops.4.1.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593495440 -> 139664412998864
	139664412998864 [label=AccumulateGrad]
	139664412972128 -> 139664412972080
	139665593494800 [label="stages.2.1.preprocessor.pre1.1.weight
 (64, 320, 1, 1)" fillcolor=lightblue]
	139665593494800 -> 139664412972128
	139664412972128 [label=AccumulateGrad]
	139664412971792 -> 139664412971360
	139665593560096 [label="stages.2.1.ops.0.1.1.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	139665593560096 -> 139664412971792
	139664412971792 [label=AccumulateGrad]
	139664412971456 -> 139664412934880
	139665593560256 [label="stages.2.1.ops.0.1.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593560256 -> 139664412971456
	139664412971456 [label=AccumulateGrad]
	139665592837744 -> 139665592836976
	139665593560416 [label="stages.2.1.ops.0.1.5.weight
 (64, 1, 5, 5)" fillcolor=lightblue]
	139665593560416 -> 139665592837744
	139665592837744 [label=AccumulateGrad]
	139665592837120 -> 139665592837264
	139665593560576 [label="stages.2.1.ops.0.1.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593560576 -> 139665592837120
	139665592837120 [label=AccumulateGrad]
	139665592836256 -> 139665592737168
	139665592836256 [label=SumBackward1]
	139665592836736 -> 139665592836256
	139665592836736 [label=StackBackward0]
	139665592837648 -> 139665592836736
	139665592837648 [label=NativeGroupNormBackward0]
	139665592837168 -> 139665592837648
	139665592837168 [label=ConvolutionBackward0]
	139664412971888 -> 139665592837168
	139664412971888 [label=ConvolutionBackward0]
	139664412972560 -> 139664412971888
	139664412972560 [label=ReluBackward0]
	139664412972272 -> 139664412972560
	139664412972272 [label=NativeGroupNormBackward0]
	139664412997712 -> 139664412972272
	139664412997712 [label=ConvolutionBackward0]
	139664412999536 -> 139664412997712
	139664412999536 [label=ConvolutionBackward0]
	139664412998912 -> 139664412999536
	139664412998912 [label=ReluBackward0]
	139664412971984 -> 139664412998912
	139664413024560 -> 139664412999536
	139665593558976 [label="stages.2.1.ops.1.0.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593558976 -> 139664413024560
	139664413024560 [label=AccumulateGrad]
	139664412999440 -> 139664412997712
	139665593557856 [label="stages.2.1.ops.1.0.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593557856 -> 139664412999440
	139664412999440 [label=AccumulateGrad]
	139664412971936 -> 139664412971888
	139665593557296 [label="stages.2.1.ops.1.0.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593557296 -> 139664412971936
	139664412971936 [label=AccumulateGrad]
	139664412972032 -> 139665592837168
	139665593560976 [label="stages.2.1.ops.1.0.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593560976 -> 139664412972032
	139664412972032 [label=AccumulateGrad]
	139665592836784 -> 139665592836736
	139665592836784 [label=NativeGroupNormBackward0]
	139664412971648 -> 139665592836784
	139664412971648 [label=ConvolutionBackward0]
	139665592837456 -> 139664412971648
	139665592837456 [label=ConvolutionBackward0]
	139664412995648 -> 139665592837456
	139664412995648 [label=ReluBackward0]
	139664413024656 -> 139664412995648
	139664413024656 [label=NativeGroupNormBackward0]
	139664413024752 -> 139664413024656
	139664413024752 [label=ConvolutionBackward0]
	139664413024848 -> 139664413024752
	139664413024848 [label=ConvolutionBackward0]
	139664413024992 -> 139664413024848
	139664413024992 [label=ReluBackward0]
	139665592837696 -> 139664413024992
	139664413024944 -> 139664413024848
	139665593559776 [label="stages.2.1.ops.1.1.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593559776 -> 139664413024944
	139664413024944 [label=AccumulateGrad]
	139664413024800 -> 139664413024752
	139665593559456 [label="stages.2.1.ops.1.1.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593559456 -> 139664413024800
	139664413024800 [label=AccumulateGrad]
	139664413024512 -> 139665592837456
	139665593364624 [label="stages.2.1.ops.1.1.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593364624 -> 139664413024512
	139664413024512 [label=AccumulateGrad]
	139664412995936 -> 139664412971648
	139665593365744 [label="stages.2.1.ops.1.1.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593365744 -> 139664412995936
	139664412995936 [label=AccumulateGrad]
	139665592836160 -> 139665592737168
	139665592836160 [label=SumBackward1]
	139664412971312 -> 139665592836160
	139664412971312 [label=StackBackward0]
	139664412998000 -> 139664412971312
	139664412998000 [label=NativeGroupNormBackward0]
	139664413024608 -> 139664412998000
	139664413024608 [label=ConvolutionBackward0]
	139664413025040 -> 139664413024608
	139664413025040 [label=ConvolutionBackward0]
	139664413025184 -> 139664413025040
	139664413025184 [label=ReluBackward0]
	139664413025328 -> 139664413025184
	139664413025328 [label=NativeGroupNormBackward0]
	139664413025424 -> 139664413025328
	139664413025424 [label=ConvolutionBackward0]
	139664413025520 -> 139664413025424
	139664413025520 [label=ConvolutionBackward0]
	139664413025664 -> 139664413025520
	139664413025664 [label=ReluBackward0]
	139665592837696 -> 139664413025664
	139664413025616 -> 139664413025520
	139665593365584 [label="stages.2.1.ops.2.0.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593365584 -> 139664413025616
	139664413025616 [label=AccumulateGrad]
	139664413025472 -> 139664413025424
	139665593365344 [label="stages.2.1.ops.2.0.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593365344 -> 139664413025472
	139664413025472 [label=AccumulateGrad]
	139664413025136 -> 139664413025040
	139665593365024 [label="stages.2.1.ops.2.0.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593365024 -> 139664413025136
	139664413025136 [label=AccumulateGrad]
	139664413025088 -> 139664413024608
	139665593367024 [label="stages.2.1.ops.2.0.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593367024 -> 139664413025088
	139664413025088 [label=AccumulateGrad]
	139665592836496 -> 139664412971312
	139665592836496 [label=NativeGroupNormBackward0]
	139664413025376 -> 139665592836496
	139664413025376 [label=ConvolutionBackward0]
	139664413024896 -> 139664413025376
	139664413024896 [label=ConvolutionBackward0]
	139664413025712 -> 139664413024896
	139664413025712 [label=ReluBackward0]
	139664413025856 -> 139664413025712
	139664413025856 [label=NativeGroupNormBackward0]
	139664413025952 -> 139664413025856
	139664413025952 [label=ConvolutionBackward0]
	139664413026048 -> 139664413025952
	139664413026048 [label=ConvolutionBackward0]
	139664413026192 -> 139664413026048
	139664413026192 [label=ReluBackward0]
	139664412971984 -> 139664413026192
	139664413026144 -> 139664413026048
	139665593366864 [label="stages.2.1.ops.2.1.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593366864 -> 139664413026144
	139664413026144 [label=AccumulateGrad]
	139664413026000 -> 139664413025952
	139665593366624 [label="stages.2.1.ops.2.1.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593366624 -> 139664413026000
	139664413026000 [label=AccumulateGrad]
	139664413025760 -> 139664413024896
	139665593366304 [label="stages.2.1.ops.2.1.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665593366304 -> 139664413025760
	139664413025760 [label=AccumulateGrad]
	139664413025232 -> 139664413025376
	139665593368304 [label="stages.2.1.ops.2.1.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665593368304 -> 139664413025232
	139664413025232 [label=AccumulateGrad]
	139665592836352 -> 139665592737168
	139665592836352 [label=SumBackward1]
	139665592836544 -> 139665592836352
	139665592836544 [label=StackBackward0]
	139664413025904 -> 139665592836544
	139664413025904 [label=MaxPool2DWithIndicesBackward0]
	139665592836256 -> 139664413025904
	139664413024368 -> 139665592836544
	139664413024368 [label=NativeGroupNormBackward0]
	139664413025568 -> 139664413024368
	139664413025568 [label=ConvolutionBackward0]
	139664413026240 -> 139664413025568
	139664413026240 [label=ConvolutionBackward0]
	139664413026384 -> 139664413026240
	139664413026384 [label=ReluBackward0]
	139664413026528 -> 139664413026384
	139664413026528 [label=NativeGroupNormBackward0]
	139664413026624 -> 139664413026528
	139664413026624 [label=ConvolutionBackward0]
	139664413026720 -> 139664413026624
	139664413026720 [label=ConvolutionBackward0]
	139664413026864 -> 139664413026720
	139664413026864 [label=ReluBackward0]
	139664412971984 -> 139664413026864
	139664413026816 -> 139664413026720
	139665592964016 [label="stages.2.1.ops.3.1.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665592964016 -> 139664413026816
	139664413026816 [label=AccumulateGrad]
	139664413026672 -> 139664413026624
	139665592963776 [label="stages.2.1.ops.3.1.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665592963776 -> 139664413026672
	139664413026672 [label=AccumulateGrad]
	139664413026336 -> 139664413026240
	139665592963456 [label="stages.2.1.ops.3.1.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665592963456 -> 139664413026336
	139664413026336 [label=AccumulateGrad]
	139664413026288 -> 139664413025568
	139665592965456 [label="stages.2.1.ops.3.1.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665592965456 -> 139664413026288
	139664413026288 [label=AccumulateGrad]
	139665592836400 -> 139665592737168
	139665592836400 [label=SumBackward1]
	139664413025280 -> 139665592836400
	139664413025280 [label=StackBackward0]
	139664413026576 -> 139664413025280
	139664413026576 [label=MaxPool2DWithIndicesBackward0]
	139665592836256 -> 139664413026576
	139664413025808 -> 139664413025280
	139664413025808 [label=NativeGroupNormBackward0]
	139664413026432 -> 139664413025808
	139664413026432 [label=ConvolutionBackward0]
	139664413026912 -> 139664413026432
	139664413026912 [label=ConvolutionBackward0]
	139664413027056 -> 139664413026912
	139664413027056 [label=ReluBackward0]
	139664413027200 -> 139664413027056
	139664413027200 [label=NativeGroupNormBackward0]
	139664413027296 -> 139664413027200
	139664413027296 [label=ConvolutionBackward0]
	139664413027392 -> 139664413027296
	139664413027392 [label=ConvolutionBackward0]
	139664413027536 -> 139664413027392
	139664413027536 [label=ReluBackward0]
	139664412971984 -> 139664413027536
	139664413027488 -> 139664413027392
	139665592966656 [label="stages.2.1.ops.4.1.1.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665592966656 -> 139664413027488
	139664413027488 [label=AccumulateGrad]
	139664413027344 -> 139664413027296
	139665592966176 [label="stages.2.1.ops.4.1.2.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665592966176 -> 139664413027344
	139664413027344 [label=AccumulateGrad]
	139664413027008 -> 139664413026912
	139665592965856 [label="stages.2.1.ops.4.1.5.weight
 (64, 1, 3, 3)" fillcolor=lightblue]
	139665592965856 -> 139664413027008
	139664413027008 [label=AccumulateGrad]
	139664413026960 -> 139664413026432
	139665592939120 [label="stages.2.1.ops.4.1.6.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139665592939120 -> 139664413026960
	139664413026960 [label=AccumulateGrad]
	139665592737600 -> 139665592737648
	139665592737600 [label=TBackward0]
	139665592836448 -> 139665592737600
	139665593368464 [label="classifier.weight
 (10, 320)" fillcolor=lightblue]
	139665593368464 -> 139665592836448
	139665592836448 [label=AccumulateGrad]
	139665592737648 -> 139665592762176
}
